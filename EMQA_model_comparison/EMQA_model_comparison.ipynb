{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QuantLet/EMQA/blob/main/EMQA_model_comparison/EMQA_model_comparison.ipynb)\n\n# EMQA_model_comparison\n\nRolling 1-step-ahead model comparison (Naive, Random Forest, Gradient Boosting, LSTM) on Romanian electricity price data with bootstrap confidence intervals. LSTM is trained using PyTorch.\n\n**Output:** `ml_model_comparison.pdf`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'none',\n",
    "    'axes.facecolor': 'none',\n",
    "    'savefig.facecolor': 'none',\n",
    "    'savefig.transparent': True,\n",
    "    'axes.grid': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size': 11,\n",
    "    'figure.figsize': (12, 6),\n",
    "})\n",
    "\n",
    "COLORS = {\n",
    "    'blue': '#1A3A6E', 'red': '#CD0000', 'green': '#2E7D32',\n",
    "    'orange': '#E67E22', 'purple': '#8E44AD', 'gray': '#808080',\n",
    "    'cyan': '#00BCD4', 'amber': '#B5853F'\n",
    "}\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    fig.savefig(name, bbox_inches='tight', transparent=True, dpi=300)\n",
    "    print(f\"Saved: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/QuantLet/EMQA/main/EMQA_model_comparison/ro_de_prices_full.csv'\n",
    "ro = pd.read_csv(url, parse_dates=['date'], index_col='date')\n",
    "print(f'Loaded {len(ro)} observations')\n",
    "print(ro.columns.tolist())\n",
    "ro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# --- Build 32-feature set predicting price CHANGES (same as EMQA_feature_importance) ---\ndata = ro.copy()\ndata['target'] = data['ro_price'].diff()  # predict daily price change\n\n# RO lagged levels and changes\nfor lag in [1, 2, 3, 5, 7, 14, 30]:\n    data[f'ro_lag_{lag}'] = data['ro_price'].shift(lag)\n    data[f'ro_diff_lag_{lag}'] = data['ro_price'].diff().shift(lag)\n\n# Rolling stats\nfor w in [7, 14, 30]:\n    data[f'ro_ma_{w}'] = data['ro_price'].shift(1).rolling(w).mean()\n    data[f'ro_std_{w}'] = data['ro_price'].shift(1).rolling(w).std()\n\n# DE cross-market features\nfor lag in [1, 7]:\n    data[f'de_lag_{lag}'] = data['de_price'].shift(lag)\ndata['spread_lag1'] = data['ro_price'].shift(1) - data['de_price'].shift(1)\n\n# Temperature\ndata['ro_temp_lag1'] = data['ro_temp_mean'].shift(1)\ndata['hdd'] = (18 - data['ro_temp_mean'].shift(1)).clip(lower=0)\ndata['cdd'] = (data['ro_temp_mean'].shift(1) - 18).clip(lower=0)\n\n# Consumption\ndata['consumption_lag1'] = data['ro_consumption'].shift(1)\ndata['consumption_lag7'] = data['ro_consumption'].shift(7)\ndata['residual_load_lag1'] = data['ro_residual_load'].shift(1)\n\n# Temporal\ndata['dow'] = data.index.dayofweek\ndata['month'] = data.index.month\ndata['weekend'] = (data.index.dayofweek >= 5).astype(int)\n\ndata = data.dropna()\nexclude = ['target', 'ro_price', 'de_price', 'gas_price',\n           'de_temp_mean', 'de_temp_max', 'de_temp_min',\n           'ro_temp_mean', 'ro_temp_max', 'ro_temp_min',\n           'ro_nuclear', 'ro_hydro', 'ro_coal', 'ro_gas',\n           'ro_wind', 'ro_solar', 'ro_consumption', 'ro_residual_load']\nfeature_cols = [c for c in data.columns if c not in exclude]\n\nprint(f\"Dataset: {len(data)} rows, {len(feature_cols)} features\")\nprint(f\"Features: {feature_cols}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rolling expanding-window forecast: predict price CHANGES, convert to levels\ninit_train = int(len(data) * 0.6)\nretrain_every = 30\n\nrf_model = None\ngb_model = None\n\n# Storage (all in price LEVELS after conversion)\nnaive_preds, rf_preds, gb_preds = [], [], []\nrf_ci_lo, rf_ci_hi = [], []\ngb_ci_lo, gb_ci_hi = [], []\nactuals, dates_out = [], []\n\nfor i in range(init_train, len(data)):\n    step = i - init_train\n\n    # Retrain RF and GB every 30 steps\n    if step % retrain_every == 0:\n        X_tr = data[feature_cols].iloc[:i].values\n        y_tr = data['target'].iloc[:i].values  # price changes\n        rf_model = RandomForestRegressor(\n            n_estimators=200, max_depth=10, random_state=42, n_jobs=-1).fit(X_tr, y_tr)\n        gb_model = GradientBoostingRegressor(\n            n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42).fit(X_tr, y_tr)\n\n    X_step = data[feature_cols].iloc[i:i+1].values\n    prev_price = data['ro_price'].iloc[i - 1]\n    actual_price = data['ro_price'].iloc[i]\n\n    # Naive: tomorrow = today (zero change)\n    naive_preds.append(prev_price)\n\n    # RF prediction: change → level\n    rf_chg = rf_model.predict(X_step)[0]\n    rf_tree_chg = np.array([t.predict(X_step)[0] for t in rf_model.estimators_])\n    rf_preds.append(prev_price + rf_chg)\n    rf_ci_lo.append(prev_price + np.percentile(rf_tree_chg, 2.5))\n    rf_ci_hi.append(prev_price + np.percentile(rf_tree_chg, 97.5))\n\n    # GB prediction: change → level\n    gb_chg = gb_model.predict(X_step)[0]\n    gb_staged = np.array([p[0] for p in gb_model.staged_predict(X_step)])\n    half = max(1, gb_model.n_estimators // 2)\n    gb_recent = gb_staged[half:]\n    gb_preds.append(prev_price + gb_chg)\n    gb_ci_lo.append(prev_price + np.percentile(gb_recent, 2.5))\n    gb_ci_hi.append(prev_price + np.percentile(gb_recent, 97.5))\n\n    actuals.append(actual_price)\n    dates_out.append(data.index[i])\n\n# Convert\nactuals = np.array(actuals)\ndates_out = pd.DatetimeIndex(dates_out)\nnaive_preds = np.array(naive_preds)\nrf_preds = np.array(rf_preds)\ngb_preds = np.array(gb_preds)\nrf_ci_lo = np.array(rf_ci_lo)\nrf_ci_hi = np.array(rf_ci_hi)\ngb_ci_lo = np.array(gb_ci_lo)\ngb_ci_hi = np.array(gb_ci_hi)\n\n# All results in levels; use R2_OOS = 1 - MSE_model/MSE_naive\nall_models = {\n    'Naive (lag-1)': naive_preds,\n    'Random Forest': rf_preds,\n    'Gradient Boosting': gb_preds,\n}\n\nresults = {}\nmse_naive = mean_squared_error(actuals, naive_preds)\nfor name, pred in all_models.items():\n    mae = mean_absolute_error(actuals, pred)\n    r2_oos = 1 - mean_squared_error(actuals, pred) / mse_naive\n    results[name] = {'MAE': mae, 'R2_OOS': r2_oos}\n    print(f\"{name:22s}  MAE={mae:.2f}  R2_OOS={r2_oos*100:.1f}%\")\n\nbest_name = min(\n    [k for k in results if k != 'Naive (lag-1)'],\n    key=lambda k: results[k]['MAE'])\nprint(f\"\\nBest tree model by MAE: {best_name}\")"
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\n\n# --- Multivariate LSTM using PyTorch ---\nLOOKBACK = 14\nHIDDEN = 64\nNUM_LAYERS = 2\nEPOCHS = 100\nLR = 0.003\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size=HIDDEN, num_layers=NUM_LAYERS):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n                           batch_first=True, dropout=0.1 if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, 1)\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :]).squeeze(-1)\n\ndef make_sequences_mv(features, targets, lookback):\n    \"\"\"Sequence for target[i] uses features[i-L+1:i+1] (includes current step).\"\"\"\n    X, y = [], []\n    for i in range(lookback - 1, len(features)):\n        X.append(features[i - lookback + 1:i + 1])\n        y.append(targets[i])\n    return np.array(X), np.array(y)\n\ndef train_lstm_mv(features, targets, lookback=LOOKBACK, epochs=EPOCHS):\n    \"\"\"Train multivariate LSTM on feature sequences.\"\"\"\n    scaler_X = StandardScaler()\n    feat_scaled = scaler_X.fit_transform(features)\n    X_seq, y_seq = make_sequences_mv(feat_scaled, targets, lookback)\n    X_t = torch.FloatTensor(X_seq)\n    y_t = torch.FloatTensor(y_seq)\n    model = LSTMModel(input_size=features.shape[1], hidden_size=HIDDEN, num_layers=NUM_LAYERS)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n    loss_fn = nn.MSELoss()\n    model.train()\n    for _ in range(epochs):\n        optimizer.zero_grad()\n        loss = loss_fn(model(X_t), y_t)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n    model.eval()\n    return model, scaler_X\n\ndef predict_lstm_mv(model, scaler_X, recent_features, lookback=LOOKBACK):\n    \"\"\"Predict next change from recent feature matrix (last lookback rows).\"\"\"\n    scaled = scaler_X.transform(recent_features[-lookback:])\n    X = torch.FloatTensor(scaled).unsqueeze(0)  # (1, lookback, n_features)\n    with torch.no_grad():\n        return model(X).item()\n\n# Rolling LSTM predictions (retrain every 30 steps, same as RF/GB)\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nlstm_model, lstm_scaler = None, None\nlstm_preds = []\nall_features = data[feature_cols].values\nall_targets = data['target'].values  # price changes\n\nfor i in range(init_train, len(data)):\n    step = i - init_train\n    if step % retrain_every == 0:\n        lstm_model, lstm_scaler = train_lstm_mv(all_features[:i], all_targets[:i])\n\n    # Predict change, convert to level\n    lstm_chg = predict_lstm_mv(lstm_model, lstm_scaler, all_features[:i+1])\n    prev_price = data['ro_price'].iloc[i - 1]\n    lstm_preds.append(prev_price + lstm_chg)\n\nlstm_preds = np.array(lstm_preds)\n\n# Ensemble: simple average of RF + GB + LSTM (level predictions)\nens_preds = (rf_preds + gb_preds + lstm_preds) / 3\n\nall_models['LSTM'] = lstm_preds\nall_models['Ensemble'] = ens_preds\n\nfor name in ['LSTM', 'Ensemble']:\n    pred = all_models[name]\n    mae = mean_absolute_error(actuals, pred)\n    r2_oos = 1 - mean_squared_error(actuals, pred) / mse_naive\n    results[name] = {'MAE': mae, 'R2_OOS': r2_oos}\n\n# Direction accuracy for each model\nprint(\"\\n=== Final Model Comparison ===\")\nfor name in ['Naive (lag-1)', 'Random Forest', 'Gradient Boosting', 'LSTM', 'Ensemble']:\n    pred = all_models[name]\n    mae = results[name]['MAE']\n    r2_oos = results[name]['R2_OOS']\n    dir_correct = np.mean(np.sign(pred - naive_preds) == np.sign(actuals - naive_preds)) * 100\n    results[name]['Direction'] = dir_correct\n    print(f\"  {name:22s}  MAE={mae:.2f}  R2_OOS={r2_oos*100:.1f}%  Dir={dir_correct:.0f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot 1: Actual vs best model forecast with CI\nif best_name == 'Random Forest':\n    best_pred, best_lo, best_hi = rf_preds, rf_ci_lo, rf_ci_hi\nelif best_name == 'Gradient Boosting':\n    best_pred, best_lo, best_hi = gb_preds, gb_ci_lo, gb_ci_hi\nelse:\n    best_pred, best_lo, best_hi = naive_preds, rf_ci_lo, rf_ci_hi\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(dates_out, actuals, color=COLORS['blue'], lw=1.5, label='Actual')\nax.plot(dates_out, best_pred, color=COLORS['red'], lw=1.5, ls='--',\n        label=f'{best_name} Forecast')\nax.fill_between(dates_out, best_lo, best_hi,\n                color=COLORS['red'], alpha=0.12, label='95% CI')\n\nax.set_xlabel('Date')\nax.set_ylabel('Price (EUR/MWh)')\nax.set_title(f'Romanian Electricity: Rolling 1-Step-Ahead ({best_name})\\n'\n             f'MAE={results[best_name][\"MAE\"]:.2f}, R²_OOS={results[best_name][\"R2_OOS\"]*100:.1f}%')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.10), frameon=False, ncol=3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot 2: Bar chart MAE and R2_OOS comparison (all models incl. LSTM + Ensemble)\nplot_order = ['Naive (lag-1)', 'Random Forest', 'Gradient Boosting', 'LSTM', 'Ensemble']\nres_df = pd.DataFrame(results).T.loc[plot_order]\nbar_colors = [COLORS['gray'], COLORS['green'], COLORS['orange'], COLORS['purple'], COLORS['red']]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# (A) MAE\nax = axes[0]\nbars = ax.bar(res_df.index, res_df['MAE'], color=bar_colors, alpha=0.8,\n              edgecolor='white', lw=1.5)\nfor bar, val in zip(bars, res_df['MAE']):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n            f'{val:.1f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\nax.set_title('(A) Mean Absolute Error', fontsize=13, fontweight='bold')\nax.set_ylabel('MAE (EUR/MWh)')\nax.tick_params(axis='x', rotation=20)\n\n# (B) R2_OOS (Coefficient of Determination vs naive)\nax2 = axes[1]\nr2_vals = res_df['R2_OOS'] if 'R2_OOS' in res_df.columns else res_df['R2']\nbars2 = ax2.bar(res_df.index, r2_vals, color=bar_colors, alpha=0.8,\n                edgecolor='white', lw=1.5)\nfor bar, val in zip(bars2, r2_vals):\n    ax2.text(bar.get_x() + bar.get_width()/2, max(val, 0) + 0.005,\n             f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\nax2.set_title('(B) Coefficient of Determination', fontsize=13, fontweight='bold')\nax2.set_ylabel('R$^2_{OOS}$')\nax2.tick_params(axis='x', rotation=20)\n\nfig.suptitle('Rolling Model Comparison: Romanian Electricity Price Forecasting',\n             fontsize=15, fontweight='bold', y=1.02)\nfig.tight_layout()\nsave_fig(fig, 'ml_model_comparison.pdf')\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}