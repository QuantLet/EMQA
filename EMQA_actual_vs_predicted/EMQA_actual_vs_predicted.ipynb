{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QuantLet/EMQA/blob/main/EMQA_actual_vs_predicted/EMQA_actual_vs_predicted.ipynb)\n\n# EMQA_actual_vs_predicted\n\nRolling 1-step-ahead RF+GB+LSTM ensemble forecast with bootstrap confidence intervals.\nLSTM is trained using PyTorch. Evaluates out-of-sample accuracy using **R²_OOS** (vs naive benchmark), RMSE, MAE, and Direction Accuracy.\n\n**Key Metric:** R²_OOS = 1 - MSE_model / MSE_naive (measures improvement over naive \"tomorrow = today\" benchmark)\n\n**Output:** `ml_actual_vs_predicted.pdf`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'none',\n",
    "    'axes.facecolor': 'none',\n",
    "    'savefig.facecolor': 'none',\n",
    "    'savefig.transparent': True,\n",
    "    'axes.grid': False,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'font.size': 11,\n",
    "    'figure.figsize': (12, 6),\n",
    "})\n",
    "\n",
    "COLORS = {\n",
    "    'blue': '#1A3A6E', 'red': '#CD0000', 'green': '#2E7D32',\n",
    "    'orange': '#E67E22', 'purple': '#8E44AD', 'gray': '#808080',\n",
    "    'cyan': '#00BCD4', 'amber': '#B5853F'\n",
    "}\n",
    "\n",
    "def save_fig(fig, name):\n",
    "    fig.savefig(name, bbox_inches='tight', transparent=True, dpi=300)\n",
    "    print(f\"Saved: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/QuantLet/EMQA/main/EMQA_actual_vs_predicted/ro_de_prices_full.csv'\n",
    "ro = pd.read_csv(url, parse_dates=['date'], index_col='date')\n",
    "print(f'Loaded {len(ro)} observations')\n",
    "print(ro.columns.tolist())\n",
    "ro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build 32-feature set predicting price CHANGES (same as EMQA_feature_importance)\ndata = ro.copy()\ndata['target'] = data['ro_price'].diff()  # predict daily price change\n\n# RO lagged levels and changes\nfor lag in [1, 2, 3, 5, 7, 14, 30]:\n    data[f'ro_lag_{lag}'] = data['ro_price'].shift(lag)\n    data[f'ro_diff_lag_{lag}'] = data['ro_price'].diff().shift(lag)\n\n# Rolling stats\nfor w in [7, 14, 30]:\n    data[f'ro_ma_{w}'] = data['ro_price'].shift(1).rolling(w).mean()\n    data[f'ro_std_{w}'] = data['ro_price'].shift(1).rolling(w).std()\n\n# DE cross-market features\nfor lag in [1, 7]:\n    data[f'de_lag_{lag}'] = data['de_price'].shift(lag)\ndata['spread_lag1'] = data['ro_price'].shift(1) - data['de_price'].shift(1)\n\n# Temperature\ndata['ro_temp_lag1'] = data['ro_temp_mean'].shift(1)\ndata['hdd'] = (18 - data['ro_temp_mean'].shift(1)).clip(lower=0)\ndata['cdd'] = (data['ro_temp_mean'].shift(1) - 18).clip(lower=0)\n\n# Consumption\ndata['consumption_lag1'] = data['ro_consumption'].shift(1)\ndata['consumption_lag7'] = data['ro_consumption'].shift(7)\ndata['residual_load_lag1'] = data['ro_residual_load'].shift(1)\n\n# Temporal\ndata['dow'] = data.index.dayofweek\ndata['month'] = data.index.month\ndata['weekend'] = (data.index.dayofweek >= 5).astype(int)\n\ndata = data.dropna()\nexclude = ['target', 'ro_price', 'de_price', 'gas_price',\n           'de_temp_mean', 'de_temp_max', 'de_temp_min',\n           'ro_temp_mean', 'ro_temp_max', 'ro_temp_min',\n           'ro_nuclear', 'ro_hydro', 'ro_coal', 'ro_gas',\n           'ro_wind', 'ro_solar', 'ro_consumption', 'ro_residual_load']\nfeature_cols = [c for c in data.columns if c not in exclude]\n\nprint(f\"Dataset: {len(data)} rows, {len(feature_cols)} features\")\nprint(f\"Features: {feature_cols}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# --- Multivariate LSTM (PyTorch) ---\nLOOKBACK = 14\nHIDDEN = 64\nNUM_LAYERS = 2\nEPOCHS = 100\nLR = 0.003\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size=HIDDEN, num_layers=NUM_LAYERS):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n                           batch_first=True, dropout=0.1 if num_layers > 1 else 0)\n        self.fc = nn.Linear(hidden_size, 1)\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :]).squeeze(-1)\n\ndef make_sequences_mv(features, targets, lookback):\n    \"\"\"Sequence for target[i] uses features[i-L+1:i+1] (includes current step).\"\"\"\n    X, y = [], []\n    for i in range(lookback - 1, len(features)):\n        X.append(features[i - lookback + 1:i + 1])\n        y.append(targets[i])\n    return np.array(X), np.array(y)\n\ndef train_lstm_mv(features, targets, lookback=LOOKBACK, epochs=EPOCHS):\n    scaler_X = StandardScaler()\n    feat_scaled = scaler_X.fit_transform(features)\n    X_seq, y_seq = make_sequences_mv(feat_scaled, targets, lookback)\n    X_t = torch.FloatTensor(X_seq)\n    y_t = torch.FloatTensor(y_seq)\n    model = LSTMModel(input_size=features.shape[1], hidden_size=HIDDEN, num_layers=NUM_LAYERS)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n    loss_fn = nn.MSELoss()\n    model.train()\n    for _ in range(epochs):\n        optimizer.zero_grad()\n        loss = loss_fn(model(X_t), y_t)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n    model.eval()\n    return model, scaler_X\n\ndef predict_lstm_mv(model, scaler_X, recent_features, lookback=LOOKBACK):\n    scaled = scaler_X.transform(recent_features[-lookback:])\n    X = torch.FloatTensor(scaled).unsqueeze(0)\n    with torch.no_grad():\n        return model(X).item()\n\n# --- Rolling expanding-window RF+GB+LSTM ensemble forecast ---\ninit_train = int(len(data) * 0.6)\nretrain_every = 30\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nrf_model, gb_model = None, None\nlstm_model, lstm_scaler = None, None\nall_features = data[feature_cols].values\nall_targets = data['target'].values  # price changes\n\nens_preds = []\nci_lo_list, ci_hi_list = [], []\nactuals, dates_out = [], []\n\nfor i in range(init_train, len(data)):\n    step = i - init_train\n\n    # Retrain every 30 steps\n    if step % retrain_every == 0:\n        X_tr = data[feature_cols].iloc[:i].values\n        y_tr = data['target'].iloc[:i].values  # price changes\n\n        rf_model = RandomForestRegressor(\n            n_estimators=200, max_depth=10, random_state=42, n_jobs=-1).fit(X_tr, y_tr)\n        gb_model = GradientBoostingRegressor(\n            n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42).fit(X_tr, y_tr)\n        lstm_model, lstm_scaler = train_lstm_mv(all_features[:i], all_targets[:i])\n\n    X_step = data[feature_cols].iloc[i:i+1].values\n    prev_price = data['ro_price'].iloc[i - 1]\n    actual_price = data['ro_price'].iloc[i]\n\n    # RF prediction: change → level\n    rf_chg = rf_model.predict(X_step)[0]\n    rf_tree_chg = np.array([t.predict(X_step)[0] for t in rf_model.estimators_])\n\n    # GB prediction: change → level\n    gb_chg = gb_model.predict(X_step)[0]\n    gb_staged = np.array([p[0] for p in gb_model.staged_predict(X_step)])\n    half = max(1, gb_model.n_estimators // 2)\n    gb_tree_chg = gb_staged[half:]\n\n    # LSTM prediction: change → level\n    lstm_chg = predict_lstm_mv(lstm_model, lstm_scaler, all_features[:i+1])\n\n    # Ensemble: average changes, then convert to level\n    ens_chg = (rf_chg + gb_chg + lstm_chg) / 3\n    ens_pred = prev_price + ens_chg\n    ens_preds.append(ens_pred)\n\n    # CI from tree bootstrap (in levels)\n    all_tree_chg = np.concatenate([rf_tree_chg, gb_tree_chg])\n    ci_lo_list.append(prev_price + np.percentile(all_tree_chg, 2.5))\n    ci_hi_list.append(prev_price + np.percentile(all_tree_chg, 97.5))\n\n    actuals.append(actual_price)\n    dates_out.append(data.index[i])\n\n# Convert\nens_preds = np.array(ens_preds)\nci_lo = np.array(ci_lo_list)\nci_hi = np.array(ci_hi_list)\nactuals = np.array(actuals)\ndates_out = pd.DatetimeIndex(dates_out)\n\n# --- Naive benchmark: tomorrow = today ---\nnaive_preds = data['ro_price'].iloc[init_train-1:-1].values\n\n# --- Metrics ---\nmae = mean_absolute_error(actuals, ens_preds)\nrmse = np.sqrt(mean_squared_error(actuals, ens_preds))\n\nmse_model = mean_squared_error(actuals, ens_preds)\nmse_naive = mean_squared_error(actuals, naive_preds)\n\n# R²_OOS = 1 - MSE_model / MSE_naive\nr2_oos = 1 - mse_model / mse_naive\n\n# Direction accuracy\nactual_dir = np.sign(actuals - naive_preds)\npred_dir = np.sign(ens_preds - naive_preds)\ndir_acc = np.mean(actual_dir == pred_dir) * 100\n\n# Naive MAE for reference\nnaive_mae = mean_absolute_error(actuals, naive_preds)\nmae_reduction = (1 - mae / naive_mae) * 100\n\nprint(\"=\" * 60)\nprint(\"   Ensemble (RF+GB+LSTM) vs Naive Forecast\")\nprint(\"=\" * 60)\nprint(f\"{'Naive MAE':<25} {naive_mae:.2f} EUR/MWh\")\nprint(f\"{'Ensemble MAE':<25} {mae:.2f} EUR/MWh\")\nprint(f\"{'MAE Reduction':<25} {mae_reduction:.0f}%\")\nprint(f\"{'RMSE':<25} {rmse:.2f} EUR/MWh\")\nprint(f\"{'R²_OOS (vs naive)':<25} {r2_oos*100:.1f}%\")\nprint(f\"{'Direction Accuracy':<25} {dir_acc:.1f}%\")\nprint(\"=\" * 60)\nif r2_oos > 0:\n    print(f\">>> Ensemble beats naive by {r2_oos*100:.1f}% R²_OOS\")\nelse:\n    print(\">>> Ensemble does NOT beat naive\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot 1: Time series actual vs predicted with CI band\nfig, ax = plt.subplots(figsize=(14, 6))\n\nax.plot(dates_out, actuals, color=COLORS['blue'], lw=1.5, label='Actual')\nax.plot(dates_out, ens_preds, color=COLORS['red'], lw=1.5, ls='--',\n        label='Ensemble Forecast')\nax.fill_between(dates_out, ci_lo, ci_hi,\n                color=COLORS['red'], alpha=0.12, label='95% CI (tree bootstrap)')\n\n# Metrics annotation\ntextstr = f'R$^2_{{OOS}}$ = {r2_oos*100:.1f}%\\nDirection = {dir_acc:.0f}%\\nBeats Naive: {\"Yes\" if r2_oos > 0 else \"No\"}'\nax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nax.set_xlabel('Date')\nax.set_ylabel('Price (EUR/MWh)')\nax.set_title('Romanian Electricity: Rolling RF+GB+LSTM Ensemble Forecast', fontsize=14, fontweight='bold')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.10), frameon=False, ncol=3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot 2: Scatter actual vs predicted with R2_OOS annotation\nfig, ax = plt.subplots(figsize=(8, 8))\n\nax.scatter(actuals, ens_preds, color=COLORS['blue'], alpha=0.3, s=15, edgecolors='none')\n\n# Perfect prediction line\nlims = [min(actuals.min(), ens_preds.min()), max(actuals.max(), ens_preds.max())]\nax.plot(lims, lims, color=COLORS['red'], ls='--', lw=1.5, label='Perfect Prediction')\n\n# Stats box\ntextstr = f'R$^2_{{OOS}}$ = {r2_oos*100:.1f}%\\nDirection = {dir_acc:.0f}%\\nMAE = {mae:.1f} EUR/MWh\\nRMSE = {rmse:.1f}'\nprops = dict(boxstyle='round,pad=0.4', facecolor='white', alpha=0.8, edgecolor=COLORS['gray'])\nax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n        verticalalignment='top', bbox=props)\n\nax.set_xlabel('Actual Price (EUR/MWh)')\nax.set_ylabel('Predicted Price (EUR/MWh)')\nax.set_title('Scatter: Actual vs Ensemble Predicted', fontsize=14, fontweight='bold')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.10), frameon=False)\n\nplt.tight_layout()\nsave_fig(fig, 'ml_actual_vs_predicted.pdf')\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}